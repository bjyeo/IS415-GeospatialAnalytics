---
title: "Hands-on Exercise 10"
author: "Bao Jie Yeo"
date: '2024-10-27'
date-modified: '2024-10-27'
execute: 
  eval: true
  echo: true
  freeze: true
---

## Overview

**Geographically weighted regression (GWR)** is a spatial statistical technique that takes into account spatial heterogeneity in the relationships between variables. In this exercise, we will attempt to learn how to build hedonic pricing models using GWR methods. The dependent variable is the resale prices of condominiums in 2015. The independent variables are either structual and locational.

## Datasets

The datasets we will use for this exercise consist of:

* URA Master Plan subzone boundary (*MP14_SUBZONE_WEB_PL*)
* condo_resale_2015 (*condo_resale_2015.csv*)

## Loading Packages

```{r}
pacman::p_load(olsrr, corrplot, ggpubr, sf, spdep, GWmodel, tmap, tidyverse, gtsummary)
```

The `GWmodel` package provides a collection of localized spatial statistical methods, namely: Geographically Weighted Regression (GWR), Geographically Weighted Principal Component Analysis (GWPCA), Geographically Weighted Clustering (GWC), and Geographically Weighted Generalized Linear Models (GWGLM). Some of which are provided in basic and robust versions.

## Geospatial Data Wrangling

### Importing Data

We import the *MP_SUBZONE_WEB_PL* shapefile using `st_read()` from the **sf** package.

```{r}
mpsz = st_read(dsn = "data/geospatial", layer = "MP14_SUBZONE_WEB_PL")
```

The geometry type is multipolygon. The mpsz simple feature object does not contain the necessary EPSG information. Hence, we will assign the EPSG code to the object.

### Upading CRS

```{r}
mpsz_svy21 <- st_transform(mpsz, 3414)
```

```{r}
st_crs(mpsz_svy21)
```

Notice that the EPSG is indicated as 3414, which means we have verified that the coordinate reference system is correct.

Next, we use `st_bbox()` to obtain the bounding box of the mpsz_svy21 object.

```{r}
st_bbox(mpsz_svy21)
```

## Aspatial Data Wrangling

### Importing Data

```{r}
condo_resale = read_csv("data/aspatial/Condo_resale_2015.csv")
```

```{r}
glimpse(condo_resale)
```

Let's examine the dataset to understand the variables.

```{r}
head(condo_resale$LONGITUDE)
```

```{r}
head(condo_resale$LATITUDE)
```

We also display the summary statistics of *condo_resale* to understand the distribution of the variables.

```{r}
summary(condo_resale)
```

### Converting Data Frame into sf object

The *condo_resale* data frame is aspatial. We need to convert it into an **sf** object for spatial analysis.
We can use the `st_as_sf()` function to convert the data frame into an **sf** object. `st_transform` is also called to convert the coordinates from WGS84 to SVY21.

```{r}
condo_resale.sf <- st_as_sf(condo_resale, coords = c("LONGITUDE", "LATITUDE"), crs = 4326) %>%
  st_transform(crs=3414)
```


```{r}
head(condo_resale.sf)
```

Interestingly, the output is in point feature data frame. 

## Exploratory Data Analysis

This section of the exercise deals with using the `ggplot2` package to perform EDA.

### Statistical Graphics

```{r}
ggplot(data = condo_resale.sf, aes(x=`SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="skyblue", alpha=0.7)
```

From the figure, we realize that there is a right skewed distribution, which indicates more condominium units were transacted at relative lower prices. 

The skewed distribution can be normalized using log transformation. 

```{r}
condo_resale.sf <- condo_resale.sf %>%
  mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))
```

Now, we plot the histogram of the log-transformed selling price.

```{r}
ggplot(data = condo_resale.sf, aes(x=`LOG_SELLING_PRICE`)) +
  geom_histogram(bins=20, color="black", fill="skyblue", alpha=0.7)
```

Notice after the log transformation, the plot is more normally distributed and less skewed. 

### Multiple Histogram Plots

We can also plot multiple histograms for the variables in the dataset. This is known as a trellis plot, and we can use the `ggarange()` function from the `ggpubr` package.

```{r}
AREA_SQM <- ggplot(data=condo_resale.sf, aes(x= `AREA_SQM`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

AGE <- ggplot(data=condo_resale.sf, aes(x= `AGE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CBD <- ggplot(data=condo_resale.sf, aes(x= `PROX_CBD`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_CHILDCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_CHILDCARE`)) + 
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, aes(x= `PROX_ELDERLYCARE`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_URA_GROWTH_AREA`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, aes(x= `PROX_HAWKER_MARKET`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, aes(x= `PROX_KINDERGARTEN`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_MRT <- ggplot(data=condo_resale.sf, aes(x= `PROX_MRT`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PARK <- ggplot(data=condo_resale.sf, aes(x= `PROX_PARK`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, aes(x= `PROX_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

PROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_TOP_PRIMARY_SCH`)) +
  geom_histogram(bins=20, color="black", fill="light blue")

ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, 
          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,
          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  
          ncol = 3, nrow = 4)
```

### Visualizing Statistical Ppoint Map

We also want to reveal the geospatial distribution of condominium resale prices. We can use the `tmap` package to create a point map.

```{r}
tmap_mode("plot")
```

First, we check if the sf object is valid.

```{r}
#| eval: false
st_is_valid(mpsz_svy21, reason = TRUE)
```

```{r}
invalid_polygons <- which(!st_is_valid(mpsz_svy21))
print(paste("Number of invalid polygons:", length(invalid_polygons)))
```

Interesting! There are a few polygons that are invalid. We can use the `st_make_valid()` function to fix the invalid polygons.

```{r}
mpsz_svy21 <- st_make_valid(mpsz_svy21)
any(!st_is_valid(mpsz_svy21))
```

Now, we can plot the visualization. We use `tm_dots()` instead of `tm_bubbles()` because we are trying to plot a point symbol map.

```{r}
tm_shape(mpsz_svy21)+
  tm_polygons() +
tm_shape(condo_resale.sf) +  
  tm_dots(col = "SELLING_PRICE",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

## Hedonic Price Modelling

### Simple Linear Regression Method

First, we build a simple linear regression using *SELLING_PRICE* as the dependent variable and *AREA_SQM* as the independent variable.

```{r}
condo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)
```

What the code above is doing is fitting a linear model to the data. The `lm()` function is used to fit linear models. The formula argument specifies the model to be fitted. 

We can return a summary table of the results using the `summary()` function.

```{r}
summary(condo.slr)
```
The output report reveals that the SELLING_PRICE can be explained by using the formula:

*y = -258121.1 + 14719x1*

The R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.

Since p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of *SELLING_PRICE.* This will allow us to infer that simple linear regression model above is a good estimator of *SELLING_PRICE.*

The **Coefficients**: section of the report reveals that the p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the $B_0$ and $B_1$ are equal to 0 will be rejected. As a results, we will be able to infer that the B0 and B1 are good parameter estimates.

To visualize the best fit curve on a scatterplot, we can incorporate `lm()` as a method function in ggplot’s geometry as shown in the code chunk below.

```{r}
ggplot(data=condo_resale.sf,  
       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +
  geom_point() +
  geom_smooth(method = lm, formula = y ~ x)
```

From the statistical plot above, it reveals a few outliers with relatively high selling prices. Though, most of the points are largely concentrated around the best fit line.


### Multiple Linear Regression

Before building a multiple regression model, it is important to ensure that the indepdent variables used are not highly correlated to each other. If these highly correlated independent variables are used in building a regression model by mistake, the quality of the model will be compromised. This phenomenon is known as multicollinearity in statistics.

To visualize the correlation matrix, we can use the `corrplot` package.

```{r}
corrplot(cor(condo_resale[, 5:23]), diag = FALSE, order = "AOE",
         tl.pos = "td", tl.cex = 0.5, method = "number", type = "upper")
```

From the matrix, we observe that **Freehold** is highly correlated to **LEASE_99YEAR**. Thus, it is advised to only include either one of them in the subsequent model building. We exclude **LEASE_99YEAR** for the model building.

### Building Hedonic Price Model using Multiple Linear Regression

Using the `lm()` function, we can build a multiple linear regression model.

```{r}
condo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + 
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + 
                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + 
                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + 
                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                data=condo_resale.sf)
summary(condo.mlr)
```

### Publication Quality Table: olsrr

From the report above, not all the independent variables are statistically significant. We can revise the model by removing these insignificant variables. 

This method uses the `ols_regress()` function from the `olsrr` package to generate a publication-quality table. This table will contain the coefficients, standard errors, t-values, p-values, and confidence intervals of the model.

```{r}
condo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + 
                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + 
                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                 data=condo_resale.sf)
ols_regress(condo.mlr1)
```

### Publication Quality Table: gtsummary

Additionally, the `gtsummary` package provides a flexible way to create publication-ready tables. We can use the `tbl_regression()` function to generate a table.

```{r}
gtsummary::tbl_regression(condo.mlr1, intercept = TRUE)
```

With gtsummary package, model statistics can be included in the report by either appending them to the report table by using `add_glance_table()` or adding as a table source note by using `add_glance_source_note()` as shown in the code chunk below.

```{r}
tbl_regression(condo.mlr1, 
               intercept = TRUE) %>% 
  add_glance_source_note(
    label = list(sigma ~ "\U03C3"),
    include = c(r.squared, adj.r.squared, 
                AIC, statistic,
                p.value, sigma))
```


**Checking for Multicolinearity**

In this section, we use the `olsrr` package to build better multiple linear regression models.

The code chunk below uses the `ols_vif_tol()` function to check for multicollinearity in the model.

```{r}
ols_vif_tol(condo.mlr1)
```

Since the VIF of the independent variables is less than 10, this indicates that there is no multicollinearity in the model.

**Testing for Non-Linearity**

In multiple linear regression, we have to test the assumption that linearity and addivitity of the relationship between dependent and independent variables.

We can use the `ols_plot_resid_fit` function to perform the linearity assumption test.

```{r}
ols_plot_resid_fit(condo.mlr1)
```

From the figure above, most data points are scattered around the 0 line, so we can safely conclude that the relationship between the dependent and independent variables is linear.

**Test for Normality Assumption**

We can also call upon the `ols_plot_resid_hist()` function to test the normality assumption.

```{r}
ols_plot_resid_hist(condo.mlr1)
```

From the residual histogram above, the residuals are normally distributed. This indicates that the normality assumption is satisfied.

We can also conduct formal statistical tests using `ols_test_normality()`.

```{r}
ols_test_normality(condo.mlr1)
```

The summary table above reveals that the p-values of the four tests are way smaller than the alpha value of 0.05. Hence we will reject the null hypothesis and infer that there is statistical evidence that the residual are not normally distributed.

**Testing for Spatial Autocorrelation**

The hedonic model we are trying to build uses geographically referenced attributes, hence it is important for us to visualize the residual of the model to check for spatial autocorrelation.

We need to convert *condo_resale.sf* from an sf data frame into a **SpatialPointsDataFrame**

```{r}
mlr.output <- as.data.frame(condo.mlr1$residuals)
```

Next, we join the newly created data frame with *condo_resale.sf*

```{r}
condo_resale.res.sf <- cbind(condo_resale.sf, 
                        condo.mlr1$residuals) %>%
rename(`MLR_RES` = `condo.mlr1.residuals`)
```

The next step is to convert the *condo_resale.res.sf* from a simple feature object into a **SpatialPointsDataFrame** object since `spdep` can only process sp conformed spatial data objects.

```{r}
condo_resale.sp <- as_Spatial(condo_resale.res.sf)
condo_resale.sp
```

We can use `tmap` to display the spatial distribution of the residuals.

```{r}
tm_shape(mpsz_svy21)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.4) +
tm_shape(condo_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tm_view(set.zoom.limits = c(11,14))
```

The figure above reveals that there is indeed spatial autocorrelation in the residuals. We can use the `spdep` package to test for spatial autocorrelation.

We can perform a Moran's I test.

First, we compute the distance-based weight matrix by using `dnearneigh()` function.

```{r}
nb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)
summary(nb)
```

Next, we compute the spatial weight matrix using the `nb2listw()` function.

```{r}
nb_lw <- nb2listw(nb, style = 'W')
summary(nb_lw)
```

Finally, we can perform the `lm.morantest()` function to test for residual spatial autocorrelation.

```{r}
lm.morantest(condo.mlr1, nb_lw)
```

The Global Moran’s I test for residual spatial autocorrelation shows that it’s p-value is less than 0.00000000000000022 which is less than the alpha value of 0.05. Hence, we will reject the null hypothesis that the residuals are randomly distributed.

Since the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution.